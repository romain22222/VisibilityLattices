\newcommand{\Kernel}[1]{\ensuremath{w_{\sigma}(#1)}}

\paragraph{Saliency-aware normal estimator.}

We propose a new normal estimator on digital surfaces,
which uses the visibility of the point of interest to define its
window of computation. Contributions of neighboring points are also
possibly weighted according to a scale $\sigma$, set as proportional
to the average visibility distance. As observed in our experiments,
and proven for $k$-visibility with $k \ge 2$ in the next section,
this average distance is no less than $\Omega(\sqrt{h})$. This new
estimator is shown multigrid convergent along digitization of smooth
shapes with same speed in the next section. Furthermore, since the
computation window is limited by the visibility, it better
approximates the geometry near sharp or salient features.

Let $V_p$ be the set of visible points from the point $p$.
Let $c_p$ be the weighted centroid of the visible points around $p$,
i.e. $c_p \coloneqq \frac{\sum_{q \in V_p} \Kernel{\|q-p\|}q}{\sum_{q \in
V_p} \Kernel{\|q-p\|}}$. We form the weighted covariance matrix
$\mathcal{V}_p$ of the points $V_p$ as:
\begin{equation}
    \mathcal{V}_p = \sum_{q \in V_p} \Kernel{\|q-p\|}(q - c_p)(q - c_p)^T.
\end{equation}
The \emph{visibility normal} $\vec{n}(p)$ of point $p$ at scale
$\sigma$ is defined as the last eigenvector of the covariance matrix
$\mathcal{V}_p$ of the visible points, corresponding to its smallest
eigenvalue. Its orientation is chosen to point in the same direction
as the average of the trivial normals to the surfels touching the
point $p$. Note that the kernel $\Kernel{\cdot}$ can be chosen to be
any positive function. We set $\sigma := \frac{1}{\#
    V_p}\sum_{q \in V_p} \|q-p\|$ to be the average visibility
  distance from $p$.

In practice, we chose to test 3 different kernels. First, the Uniform
kernel ($\Kernel{x} = \mathbbm{1}_{x \leq 2\sigma}$) aims at weighting
everything the same, whatever its distance to p. The Gaussian kernel
($\Kernel{x} = e^{-\frac{x^2}{2\sigma^2}}$) aims at weighting closer
data more than further ones, arguing closer data represents better the
information than further one. Finally, the Ring kernel ($\Kernel{x} =
\frac{x}{2\sigma^2}e^{-\frac{x^2}{2\sigma^2}}$) aims at weighting data
in middle range more than the rest, trying to extrapolate the
tangential information of the visibility instead of its
proximity. In \RefFigure{fig:errors-normals-showcase-ellipsoids}, we see
that the Uniform kernel makes a lot more errors than the other two,
while the Gaussian and Ring kernels have similar errors, so for the
normal error tests we only tested on the latter two kernels. We also
tested the influence of the star parameter $k$, which controls how
thick the visibility cone is, since convergence is proven only
starting from $k=2$. However we did not see a significant influence on
the normal estimation, so we kept it to 1 for all the tests.

\newcommand{\formatImageFigNormalesScale}[1]{%
    \includegraphics[width=0.4\columnwidth]{./pictures/figNormalesScale/#1}%
}
\newcommand{\raisingFNST}[1]{\raisebox{18mm}{#1}}
\colorlet{MyLG}{gray!20}
\begin{figure*}
    \centering
    \begin{tabular}{|c||c|c|c|c|}
        \hline
        $h$ & 0.11 & 0.065 & 0.0325 & 0.01625 \\
        \hline
        \hline
        \rowcolor{MyLG}
        \raisingFNST{$II$} &
        \formatImageFigNormalesScale{4s/ellipsoids-II} &
        \formatImageFigNormalesScale{3s/ellipsoids-II} &
        \formatImageFigNormalesScale{2s/ellipsoids-II} &
        \formatImageFigNormalesScale{1s/ellipsoids-II} \\
        \hline
        \rowcolor{MyLG}
        \raisingFNST{$PCA$} &
        \formatImageFigNormalesScale{4s/ellipsoids-Mitra} &
        \formatImageFigNormalesScale{3s/ellipsoids-Mitra} &
        \formatImageFigNormalesScale{2s/ellipsoids-Mitra} &
        \formatImageFigNormalesScale{1s/ellipsoids-Mitra} \\
        \hline
        \rowcolor{MyLG}
        \raisingFNST{$VN_U$} &
        \formatImageFigNormalesScale{4s/ellipsoids-VNU} &
        \formatImageFigNormalesScale{3s/ellipsoids-VNU} &
        \formatImageFigNormalesScale{2s/ellipsoids-VNU} &
        \formatImageFigNormalesScale{1s/ellipsoids-VNU} \\
        \hline
        \rowcolor{MyLG}
        \raisingFNST{$VN_G$} &
        \formatImageFigNormalesScale{4s/ellipsoids-VNG} &
        \formatImageFigNormalesScale{3s/ellipsoids-VNG} &
        \formatImageFigNormalesScale{2s/ellipsoids-VNG} &
        \formatImageFigNormalesScale{1s/ellipsoids-VNG} \\
        \hline
        \rowcolor{MyLG}
        \raisingFNST{$VN_R$} &
        \formatImageFigNormalesScale{4s/ellipsoids-VNR} &
        \formatImageFigNormalesScale{3s/ellipsoids-VNR} &
        \formatImageFigNormalesScale{2s/ellipsoids-VNR} &
        \formatImageFigNormalesScale{1s/ellipsoids-VNR} \\
        \hline
    \end{tabular}
    \includegraphics{./pictures/figNormalesScale/legend}
    \caption{Normal error on the Ellipsoid Union shape for different methods and different gridsteps.
    VN stands for Visibility Normal, and the subscript stands for the kernel used (Uniform, Gaussian or Ring).
    The error is colored from white (0 rad) to deep red (0.1 rad $\approx$ 5.73\textdegree). II normals are computed using a radius of
    $r=\frac{7}{6}$, PCA with a discrete radius of $\sigma = 2h^{-\frac{1}{2}}$, while our
    normal estimator are computed based on the mean visibility distance}
    \label{fig:errors-normals-showcase-ellipsoids}
\end{figure*}

\paragraph{Experimental validation.}

Figure~\ref{fig:normals-estimation} shows an example
of our normals computed with both kernels compared
to normals computed with the integral invariant (II) estimator~\cite{Lachaud:2017-lnm}
and the PCA estimator~\cite{mitra:2003-acm}.
In order to compare quantitatively the different normal estimators, we compute the
root-mean-square error (RMSE) and the maximum error $E_{\max}$ of the
normals computed on a digital smooth surface with respect to the normals computed
on the continuous surface (Figure~\ref{fig:errors-normals}). We use 4 different digital smooth surfaces (ellipsoid,
goursat, rcube and sphere9). We see that the RMSE and $E_{\max}$ errors of our
estimator are convergent with respect to the grid resolution with a
rate of convergence of $h^{\frac{2}{3}}$ for the RMSE and $h^{\frac{1}{2}}$ for
$E_{\max}$. The ring kernel seems to be slightly better than the Gaussian kernel,
but they have the same convergence rates, which indicates that the choice of the kernel
does not influence the convergence of the normal estimator, but only the constant in front of the error.

Figure~\ref{fig:errors-normals} also shows these errors computed on 2 piecewise smooth shapes
(cubesphere and double ellipsoid). In order to capture the behavior of the normal estimators,
we ignore the points that are closer than $4h$ to the singularities for Cubesphere and $8h$ for the Ellipsoids Union (see~\ref{fig:singularities-showcase}).
We see that the RMSE and $E_{\max}$ are much better than both II and PCA, with a convergence rate at $h^{\frac{2}{3}}$ for RMSE and $h^{\frac{1}{2}}$ for $E_{\max}$,
both failing to accomodate the saillency. It however doesn't seem
to have a better kernel between the ring and the gaussian kernel, sometimes one is better than the other, but
it really depends on what kind of shapes we are facing with. We can also notice that the singularity zone on the Ellipsoids Union
might be a bit too small for smaller gridsteps, since the error shown in~\ref{fig:errors-normals-showcase-ellipsoids} seems to accumulate around
the singularity for all estimators.

\begin{figure*}
    \centering
    \input{error-normals.tikz}
    \caption{Error RMSE and $E_{\max}$ as a function of grid resolution on 6 different shapes}
    \label{fig:errors-normals}
\end{figure*}

\newcommand{\formatImageFigNormalesShow}[1]{%
    \includegraphics[width=0.26\textwidth]{./pictures/figNormalesDisplay/#1}%
}

\newcommand{\nameDisplayFigNormal}[1]{\raisebox{18mm}{#1}}

\begin{figure*}
    \centering
    \begin{tabular}{|c||c|c|c|}
        \hline
        Shape & II & PCA & Ours \\
        \hline
        \nameDisplayFigNormal{Cps} &
        \formatImageFigNormalesShow{cps-IIN-flat} &
        \formatImageFigNormalesShow{cps-Mitra-flat} &
        \formatImageFigNormalesShow{cps-VN-flat} \\
        \hline
        \nameDisplayFigNormal{Dragon} &
        \formatImageFigNormalesShow{dragon-IIN-flat} &
        \formatImageFigNormalesShow{dragon-Mitra-flat} &
        \formatImageFigNormalesShow{dragon-VN-flat} \\
        \hline
        \nameDisplayFigNormal{Filigree} &
        \formatImageFigNormalesShow{filigree-IIN-flat} &
        \formatImageFigNormalesShow{filigree-Mitra-flat} &
        \formatImageFigNormalesShow{filigree-VN-flat} \\
        \hline
        \raisebox{9mm}{Tie Fighter} &
        \formatImageFigNormalesShow{tie-IIN-flat} &
        \formatImageFigNormalesShow{tie-Mitra-flat} &
        \formatImageFigNormalesShow{tie-VN-flat} \\
        \hline
    \end{tabular}
    \caption{Examples of normals computed on several shapes
    as a color map. First set displays II normals, second set
    uses the PCA algorithm, and last set uses our visibility algorithm
    to compute them. The normals are colored according to their
    orientation from the camera's angle as $0.5 \cdot (n_i + \mathbbm{1})$.
    ``Integral Invariant normals`` are computed using a radius of
        $r=4.5$, PCA with a discrete radius of 4 (empirical), while our
        normal estimator are computed using a deviation of $\sigma=4$, and so a
        radius of $r=2\sigma=8$, with a star of 1 and with a weighted kernel.}
    \label{fig:normals-estimation}
\end{figure*}

\begin{figure}
    % 3 pictures side by side, with a name under each, singularities in red
    \centering
    \begin{tabular}{c c}
        \includegraphics[width=0.3\columnwidth]{pictures/singularities/cubesphereS} &
        \includegraphics[width=0.3\columnwidth]{pictures/singularities/double_ellipsoidS} \\
        Cubesphere & Ellipsoids Union \\
    \end{tabular}
    \caption{Singularities (in red) of the piecewise smooth shapes used to
    compute the errors in Figure~\ref{fig:errors-normals}.
    Both shapes are digitized at gridstep $h=0.25$.}
    \label{fig:singularities-showcase}
\end{figure}

%% Then, to compute the curvatures, we use the Corrected Normal Current
%% estimator~\cite{lachaud:2022-dcg} which are based on the normals computed
%% above.

\paragraph{Application to curvature estimation.}
In order to illustrate better that the visibility normals better
take into account the sharp features of a digital surface while
staying meaningfull in digitization of smooth parts, we compare
the curvature estimates induced by different discrete normal
estimators. We exploit the \emph{Corrected Normal Current (CNC)
    estimator}, whose theory is detailed in~\cite{lachaud:2022-dcg}
and which can estimate all kinds of curvatures given positions and
normals. It produces state-of-the-art curvature estimates on
polygonal surfaces~\cite{lachaud:2020-cgf}, point clouds~\cite{lachaud:2023-cgf}, and even outperforms Integral Invariant
curvature estimates~\cite{coeurjolly:2014-cviu} on digital
surfaces~\cite{lachaud:2022-dcg}. Its implementation is also
available in    \href{https://dgtal-team.github.io/doc-nightly/moduleCurvatureMeasures.html}{\textsc{DGtal}}.

Figure~\ref{fig:fig-curvatures} displays the results of CNC curvature
estimations on the digitization of a piecewise smooth shape
``Talking D20'', taken from
\href{https://ten-thousand-models.appspot.com/detail.html?file_id=1533028}{Thingi10K}.
We compare the differences obtained by just changing the discrete
normal estimator (available in the \textsc{DGtal} library):
(column II normals) Integral Invariant normals with $r=6$, (column
CTriv normals) convolved trivial normals with $r=6$, (column
Visibility normals) our proposed estimator (VN) according to
the mean visibility distance. These parameters were
chosen so that the respective computation windows of the different
estimators are approximately the same.

The II estimator is good along digitization of smooth or flat parts of
the dice, but presents curious artefacts near sharp features
induced by the hollowed out numbers: this is due to the nature of
II normal estimates, which computes a PCA of a ball centered on
the point of interest and may include points on the other side of
the saliencies.

The CTriv estimator only performs averaging of trivial normal
directions along the surface. It behaves much better than II near
features (although it tends to smooth curvatures) since it does
not use information from across the gap. However, some oscillations
of the curvatures are distinguishable especially along the flat
parts, and some curvature estimates are erroneous on smoother
parts (like the edge above 6 for $\kappa_2$ or several vertices of the dice).

The VN estimator takes the best of the two previous approaches. It
remains precise and stable on smooth and flat regions, while
perfectly delineating sharp features and holes, whatever the kind
of estimated curvatures. Its drawback is the computation time,
which is $52s$ for a maximal visibility distance $8$, compared to
$2.3$s for II and $0.1s$ for CTriv. Note that the computation time
falls back to $33s$ for a maximal distance of $6$ (and same
$\sigma=4$) while the result is visually indistinguishable.


\newcommand{\MyZoom}[1]{%
    \begin{tikzpicture}[spy using outlines={circle,magnification=1.8,size=2cm,connect spies}]
    \node[inner sep=0pt] {\pgfimage[width=0.2\linewidth]{#1}};
    \spy[overlay,blue] on (0.3,-0.18) in node at (-0.7,-0.7);
    \end{tikzpicture}}
% \newcommand{\MyZoom}[1]{\includegraphics[width=0.3\textwidth]{#1}}

% insert variable for raisebox size
\newcommand{\raiseboxsize}{10mm}

\begin{figure*}
    \centering
    \begin{tabular}{|c||c|c|c|c|}
        \hline
        & II & CTriv & PCA & Ours \\ \hline \hline
        \raisebox{\raiseboxsize}{$\kappa_1$} &
        \MyZoom{pictures/figCurvDisplay/d20-256-II-K1} &
        \MyZoom{pictures/figCurvDisplay/d20-256-CTriv-K1}&
        \MyZoom{pictures/figCurvDisplay/d20-256-Mitra-K1}&
        \MyZoom{pictures/figCurvDisplay/d20-256-VN-K1}\\ \hline
        \raisebox{\raiseboxsize}{$\kappa_2$} &
        \MyZoom{pictures/figCurvDisplay/d20-256-II-K2} &
        \MyZoom{pictures/figCurvDisplay/d20-256-CTriv-K2}&
        \MyZoom{pictures/figCurvDisplay/d20-256-Mitra-K2}&
        \MyZoom{pictures/figCurvDisplay/d20-256-VN-K2}\\ \hline
        \raisebox{\raiseboxsize}{$H$} &
        \MyZoom{pictures/figCurvDisplay/d20-256-II-H} &
        \MyZoom{pictures/figCurvDisplay/d20-256-CTriv-H}&
        \MyZoom{pictures/figCurvDisplay/d20-256-Mitra-H}&
        \MyZoom{pictures/figCurvDisplay/d20-256-VN-H}\\ \hline
        \raisebox{\raiseboxsize}{$G$} &
        \MyZoom{pictures/figCurvDisplay/d20-256-II-G} &
        \MyZoom{pictures/figCurvDisplay/d20-256-CTriv-G}&
        \MyZoom{pictures/figCurvDisplay/d20-256-Mitra-G}&
        \MyZoom{pictures/figCurvDisplay/d20-256-VN-G}\\ \hline
    \end{tabular}
    \caption{\label{fig:fig-curvatures}Estimation of curvatures using
    Corrected Normal Current estimators \cite{lachaud:2022-dcg}
    with a measure of radius $2$ for the shape ``Dice-20'' of
    Thingi10K database at resolution $256^3$. This estimator is
    parameterized by an input normal estimator: first column by
    ``Integral Invariant normals'' (radius $r=6$), second column
    by ``Convolved Trivial normals'' (radius $k=6$), third by
    ``PCA Normals`` (radius $r=4$), fourth column by our presented
    normal estimator. Per row are displayed the estimated curvatures
    in order: first and second principal curvatures $\kappa_1$ and $\kappa_2$,
    mean curvature $H$, Gaussian curvature $G$. The range of curvature
    between deep blue and deep red is $\lbrack -0.1, 0.1 \rbrack$ with white
    color equal to $0$.}
\end{figure*}
